{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc033624",
   "metadata": {},
   "source": [
    "# PINN Procedure\n",
    "\n",
    "By Zeynep SaÄŸlam, 2024\n",
    "\n",
    "PINN stands for physics-based neural networks and is often used in scientific calculations, especially in solving differential equations. This procedure was developed as an alternative to traditional numerical analysis methods and is generally effective when data is sparse, noisy, or incomplete. In this notebook, the procedure required to create a Physics-informed neural network will be explained.\n",
    "\n",
    "### Steps to follow:\n",
    "1) Importing Required Libraries\n",
    "2) Dataset Creation (Optional)\n",
    "3) Definition of Neural Network Architecture\n",
    "4) Creation of Physics-Informed Loss Functions\n",
    "5) Main Program &  Plotting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d6126",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "\n",
    "After the necessary libraries are installed, version control of the required libraries is performed. This process ensures that the requirements are notified if the program to be implemented is shared. It is not a mandatory procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3930db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.14.0\n",
      "Torch version: 2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#or/both\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import seaborn as sns \n",
    "import codecs, json\n",
    "\n",
    "tf.keras.backend.set_floatx('float64') # float64 as default\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Torch version: {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71932b",
   "metadata": {},
   "source": [
    "## 2. Dataset Creation (Optional)\n",
    "\n",
    "When creating a PINN model, it is not always necessary to create a data set. Sometimes experimental/observational data and sometimes data produced in accordance with the differential equation are used to train the model. At this stage, it is decided how to meet the data set required to train the model and added to the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b584ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data is ready in a file, file reading operations are performed.\n",
    "# This processing varies depending on the size and type of the data set, but basically;\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "file_name = 'dataset.npy' # \n",
    "\n",
    "with open('dataset.npy', 'rb') as f:\n",
    "    inputs, outputs = np.load(f)\n",
    "    \n",
    "\"\"\"This particular usage is usually used to read the contents of a file (with the \".npy\" extension)\n",
    "created by the NumPy library and containing Numpy arrays. The \"rb\" mode indicates that the file contains \n",
    "binary data. These types of files are often used to store large amounts of digital data.\"\"\"\n",
    "\n",
    "\n",
    "# if data is to be created,\n",
    "\n",
    "\n",
    "#from pyDOE import lhs   already did this on top\n",
    "\n",
    "# Size of parameter space\n",
    "num_samples = 100 # Number of samples\n",
    "num_dimensions = 3 # Dimension of the parameter space\n",
    "\n",
    "# Latin Hypercube Sampling \n",
    "lhs_samples = lhs(num_dimensions, samples=num_samples, criterion=\"maximin\")\n",
    "\n",
    "# Scaling values in the parameter space (for example, between 0 and 1)\n",
    "param_min = 0\n",
    "param_max = 1\n",
    "scaled_lhs_samples = param_min + lhs_samples * (param_max - param_min)\n",
    "\n",
    "# You can perform another operation using the obtained samples\n",
    "# For example, calculating a function on combinations of parameters\n",
    "\n",
    "# As an example, let's define a function (for example, a function that calculates the sum of the parameters)\n",
    "def sample_function(parameters):\n",
    "     return np.sum(parameters)\n",
    "\n",
    "# Calculate the function value of each sample\n",
    "function_values = np.apply_along_axis(sample_function, 1, scaled_lhs_samples)\n",
    "\n",
    "# Print results\n",
    "print(\"Data Set Created with Latin Hypercube Sampling:\")\n",
    "print(\"Parameters:\", scaled_lhs_samples)\n",
    "print(\"Function Values:\", function_values)\n",
    "\n",
    "\n",
    "# If this  transformed in a function\n",
    "\n",
    "def trainingdata(N_u,N_f,N_v):\n",
    "    \n",
    "    #It is examined here in 2 dimensions, but the number of dimensions can be arranged as desired.\n",
    "    \n",
    "    leftedge_x = np.hstack((X[:,0][:,None], Y[:,0][:,None]))\n",
    "    leftedge_u = usol[:,0][:,None]\n",
    "    \n",
    "    rightedge_x = np.hstack((X[:,-1][:,None], Y[:,-1][:,None]))\n",
    "    rightedge_u = usol[:,-1][:,None]\n",
    "    \n",
    "    topedge_x = np.hstack((X[0,:][:,None], Y[0,:][:,None]))\n",
    "    topedge_u = usol[0,:][:,None]\n",
    "    \n",
    "    bottomedge_x = np.hstack((X[-1,:][:,None], Y[-1,:][:,None]))\n",
    "    bottomedge_u = usol[-1,:][:,None]\n",
    "    \n",
    "    all_X_u_train = np.vstack([leftedge_x, rightedge_x, bottomedge_x, topedge_x])\n",
    "    all_u_train = np.vstack([leftedge_u, rightedge_u, bottomedge_u, topedge_u])  \n",
    "     \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(all_X_u_train.shape[0], N_u+N_v, replace=False) \n",
    "    \n",
    "    X_u_train = all_X_u_train[idx[0:N_u], :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = all_u_train[idx[0:N_u],:]      #choose corresponding u\n",
    "    \n",
    "    X_boundary_val = all_X_u_train[idx[N_u+1:N_u+N_v], :] #choose indices from  set 'idx' (x,t)\n",
    "    u_boundary_val = all_u_train[idx[N_u+1:N_u+N_v],:]      #choose corresponding u\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    X_f = lb + (ub-lb)*lhs(2,N_f+N_v) \n",
    "    X_f_train = np.vstack((X_f[0:N_f,:], X_u_train)) # append training points to collocation points \n",
    "    X_interior_val = X_f[N_f:N_f+N_v,:]\n",
    "    \n",
    "    return X_f_train, X_u_train, u_train, X_interior_val, X_boundary_val, u_boundary_val  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc84a3f",
   "metadata": {},
   "source": [
    "## 3. Definition of Neural Network Architecture\n",
    "\n",
    "Artificial neural network architecture consists of an input layer, one or more hidden layers and an output layer. Each layer consists of neurons, and these neurons are connected to each other by weights and activation functions. The input layer represents the features in the data set, the hidden layers are used to understand and learn the data, and the output layer produces the results. Neurons multiply input information by weights, create the sum, and then pass it to an activation function to produce the output. This architecture has a wide range of applications for solving various problems and learning data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ad8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this useful function uses tensorflow librariy\n",
    "\n",
    "def generate_PINN(n,L):  \n",
    "\n",
    "    #generate an array representing the network architecture\n",
    "    layers = np.ones(1+L).astype(int)*n # 1 input layer + L hidden layers\n",
    "    layers[0] = 2\n",
    "    layers[-1] = 1\n",
    "\n",
    "    #layers = [2, n1, n2, n3,........., n_L] #Network Configuration \n",
    "\n",
    "    PINN = tf.keras.Sequential()\n",
    "\n",
    "    PINN.add(tf.keras.layers.InputLayer(input_shape=(layers[0],),name=\"input_layer\", dtype = 'float64'))\n",
    "    \n",
    "    PINN.add(tf.keras.layers.Lambda(lambda X: 2*(X - lb)/(ub - lb) - 1)) \n",
    "    \n",
    "    initializer = 'glorot_normal'     \n",
    "    \n",
    "    for l in range (len(layers)-2):\n",
    "\n",
    "      # Xavier Initialization  \n",
    "      PINN.add(tf.keras.layers.Dense(layers[l+1],kernel_initializer=initializer, bias_initializer='zeros',\n",
    "                                  activation = tf.nn.tanh, name = \"layer\" + str(l+1), dtype = 'float64'))\n",
    "        \n",
    "    PINN.add(tf.keras.layers.Dense(layers[-1],kernel_initializer=initializer, bias_initializer='zeros',\n",
    "                          activation = None, name = \"output_layer\" , dtype = 'float64'))    \n",
    "        \n",
    "    return PINN  \n",
    "\n",
    "# If you use Pytorch;\n",
    "\n",
    "def generate_PINN(n, L, lb, ub):\n",
    "    # Generate an array representing the network architecture\n",
    "    layers = [2] + [n] * L + [1]  # Network Configuration\n",
    "    layers = [int(x) for x in layers]\n",
    "\n",
    "    class PINN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(PINN, self).__init__()\n",
    "            self.input_layer = nn.Linear(layers[0], layers[1])\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "            # Xavier Initialization\n",
    "            for l in range(1, len(layers) - 2):\n",
    "                setattr(self, f\"layer{l}\", nn.Linear(layers[l], layers[l + 1]))\n",
    "\n",
    "            self.output_layer = nn.Linear(layers[-2], layers[-1])\n",
    "\n",
    "        def forward(self, X):\n",
    "            X = self.input_layer(X)\n",
    "            X = self.activation(X)\n",
    "\n",
    "            for l in range(1, len(layers) - 2):\n",
    "                X = getattr(self, f\"layer{l}\")(X)\n",
    "                X = self.activation(X)\n",
    "\n",
    "            X = self.output_layer(X)\n",
    "\n",
    "            return X\n",
    "\n",
    "    # Create the model\n",
    "    PINN_model = PINN()\n",
    "\n",
    "    # Instead of using a Lambda layer for normalizing input data, we directly perform normalization\n",
    "    def normalize(x):\n",
    "        return 2 * (x - lb) / (ub - lb) - 1\n",
    "\n",
    "    return PINN_model, normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e7c94",
   "metadata": {},
   "source": [
    "## 4. Creation of Physics-Informed Loss Functions\n",
    "\n",
    "PINNs generally use two types of loss functions: PDE Loss and Boundary Loss. PDE Loss is used to solve differential equations and helps the model represent its behavior in a particular physical system. This loss function ensures that the model simulates the solution to the terms on the right-hand side of the differential equations as accurately as possible. Boundary Loss, on the other hand, ensures that the model behaves correctly under certain bounds or limiting conditions. Both types of loss are used in training the model, increasing its fit to observed data and allowing it to mimic physical equations more effectively. In this way, PINNs can not only fit the measured data but also accurately model the physical behavior inside the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_loss(model, input_data):\n",
    "    # Here, a term expressing the differential equation is obtained from the output of the model\n",
    "    # For example: differential_term = model(input_data) - some_derivative(input_data)\n",
    "\n",
    "    differential_term = model(input_data) - some_derivative(input_data)\n",
    "\n",
    "    # Loss function used to minimize the differential term\n",
    "    return tf.reduce_mean(tf.square(differential_term))\n",
    "\n",
    "def boundary_loss(model, boundary_data):\n",
    "\n",
    "    # Here a term is obtained that expresses the difference between the output of the model and the correct values of the limits\n",
    "    # For example: boundary_term = model(boundary_data) - some_boundary_values(boundary_data)\n",
    "\n",
    "    boundary_term = model(boundary_data) - some_boundary_values(boundary_data)\n",
    "\n",
    "    # Loss function used to minimize the difference with the true values of the limits\n",
    "    return tf.reduce_mean(tf.square(boundary_term))\n",
    "\n",
    "def total_loss(model, input_data, boundary_data):\n",
    "\n",
    "    # Total loss is the sum of PDE Loss and Boundary Loss\n",
    "\n",
    "    total_loss = pde_loss(model, input_data) + boundary_loss(model, boundary_data)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0306312",
   "metadata": {},
   "source": [
    "## 5. Main Program &  Plotting Results\n",
    "\n",
    "At this stage, after all the results are obtained, it is analyzed whether the program serves the desired purpose. Analysis of the results is basically done through three tables;\n",
    "\n",
    "1. The results of the data with physical equations \n",
    "2. The results of the created model \n",
    "3. The absolute margin of error between these two results\n",
    "\n",
    "If the results are not satisfactory, all procedures, equations and approaches should be reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solutionplot(u_pred,X_u_train,u_train):\n",
    "\n",
    "    #Ground truth\n",
    "    fig_1 = plt.figure(1, figsize=(18, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.pcolor(x_1, x_2, physics_sol, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_1$', fontsize=18)\n",
    "    plt.ylabel(r'$x_2$', fontsize=18)\n",
    "    plt.title('Ground Truth $u(x_1,x_2)$', fontsize=15)\n",
    "\n",
    "\n",
    "    # Prediction\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.pcolor(x_1, x_2, prediction , cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_1$', fontsize=18)\n",
    "    plt.ylabel(r'$x_2$', fontsize=18)\n",
    "    plt.title('Predicted $\\hat u(x_1,x_2)$', fontsize=15)\n",
    "\n",
    "    # Error\n",
    "    \n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.pcolor(x_1, x_2, np.abs(physics_sol - prediction), cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_1$', fontsize=18)\n",
    "    plt.ylabel(r'$x_2$', fontsize=18)\n",
    "    plt.title(r'Absolute error $|u(x_1,x_2)- \\hat u(x_1,x_2)|$', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('Results.png', dpi = 500, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "\"\"\" MAIN PROGRAM \"\"\"\n",
    "\n",
    "N_u = 400 #Total number of data points for the function\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "# Training data\n",
    "X_f_train_np_array, X_u_train_np_array, u_train_np_array = trainingdata(N_u,N_f)\n",
    "\n",
    "'Convert to tensor and send to GPU'\n",
    "X_f_train = torch.from_numpy(X_f_train_np_array).float()\n",
    "X_u_train = torch.from_numpy(X_u_train_np_array).float()\n",
    "u_train = torch.from_numpy(u_train_np_array).float()\n",
    "X_u_test_tensor = torch.from_numpy(X_u_test).float()\n",
    "u = torch.from_numpy(u_true).float()\n",
    "f_hat = torch.zeros(X_f_train.shape[0],1)\n",
    "\n",
    "PINN=generate_PINN(50,3)   #3 hidden layers, 50 neurons\n",
    "\n",
    "'Neural Network Summary'\n",
    "\n",
    "print(PINN)\n",
    "\n",
    "params = list(PINN.parameters())\n",
    "\n",
    "'''Optimization'''\n",
    "\n",
    "'L-BFGS Optimizer'\n",
    "\n",
    "'Adam Optimizer'\n",
    "\n",
    "optimizer = optim.Adam(PINN.parameters(), lr=0.001,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "max_iter = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(max_iter):\n",
    "\n",
    "    loss = PINN.loss(X_u_train, u_train, X_f_train)\n",
    "           \n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    \n",
    "    loss.backward() #backprop\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % (max_iter/10) == 0:\n",
    "\n",
    "        error_vec, _ = PINN.test()\n",
    "\n",
    "        print(loss,error_vec)\n",
    "    \n",
    "    \n",
    "elapsed = time.time() - start_time                \n",
    "print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "\n",
    "''' Model Accuracy ''' \n",
    "error_vec, prediction = PINN.test()\n",
    "\n",
    "print('Test Error: %.5f'  % (error_vec))\n",
    "\n",
    "\n",
    "''' Solution Plot '''\n",
    "solutionplot(prediction,X_u_train,u_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
